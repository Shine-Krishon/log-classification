import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
import joblib
import time

def train_20k_model():
    """
    Train high-performance model using the full 20K dataset
    """
    print("=== Training on Full 20K Dataset ===")
    
    # Load the original 20K dataset
    df = pd.read_csv('logs_chunk1.csv')
    print(f"Dataset size: {len(df):,} samples")
    print(f"Classes: {df['target_label'].nunique()}")
    print(f"Class distribution:")
    print(df['target_label'].value_counts())
    
    # Prepare features and targets
    X = df['log_message']
    y = df['target_label']
    
    # Split into train/validation/test (70/15/15)
    print("\n=== Creating Data Splits ===")
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.15, random_state=42, stratify=y
    )
    
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp  # 0.176 * 0.85 ‚âà 0.15
    )
    
    print(f"Train set: {len(X_train):,} samples")
    print(f"Validation set: {len(X_val):,} samples") 
    print(f"Test set: {len(X_test):,} samples")
    
    # Model 1: Optimized TF-IDF + Logistic Regression
    print("\n=== Training TF-IDF + Logistic Regression ===")
    
    start_time = time.time()
    
    # Create pipeline with optimized parameters for larger dataset
    lr_pipeline = Pipeline([
        ('tfidf', TfidfVectorizer(
            max_features=10000,  # More features for larger dataset
            ngram_range=(1, 2),
            stop_words='english',
            min_df=2,  # Ignore very rare terms
            max_df=0.95  # Ignore very common terms
        )),
        ('lr', LogisticRegression(
            max_iter=1000,
            random_state=42,
            C=1.0
        ))
    ])
    
    # Train the model
    lr_pipeline.fit(X_train, y_train)
    lr_train_time = time.time() - start_time
    
    # Evaluate Logistic Regression
    val_pred_lr = lr_pipeline.predict(X_val)
    val_acc_lr = accuracy_score(y_val, val_pred_lr)
    
    test_pred_lr = lr_pipeline.predict(X_test)
    test_acc_lr = accuracy_score(y_test, test_pred_lr)
    
    print(f"Training time: {lr_train_time:.1f} seconds")
    print(f"Validation accuracy: {val_acc_lr:.4f}")
    print(f"Test accuracy: {test_acc_lr:.4f}")
    
    # Model 2: Random Forest (optimized for large dataset)
    print("\n=== Training Random Forest ===")
    
    start_time = time.time()
    
    rf_pipeline = Pipeline([
        ('tfidf', TfidfVectorizer(
            max_features=5000,  # Fewer features for RF to avoid overfitting
            ngram_range=(1, 2),
            stop_words='english',
            min_df=2,
            max_df=0.95
        )),
        ('rf', RandomForestClassifier(
            n_estimators=100,
            max_depth=20,
            min_samples_split=10,  # Prevent overfitting on large dataset
            min_samples_leaf=5,
            random_state=42,
            n_jobs=-1
        ))
    ])
    
    rf_pipeline.fit(X_train, y_train)
    rf_train_time = time.time() - start_time
    
    # Evaluate Random Forest
    val_pred_rf = rf_pipeline.predict(X_val)
    val_acc_rf = accuracy_score(y_val, val_pred_rf)
    
    test_pred_rf = rf_pipeline.predict(X_test)
    test_acc_rf = accuracy_score(y_test, test_pred_rf)
    
    print(f"Training time: {rf_train_time:.1f} seconds")
    print(f"Validation accuracy: {val_acc_rf:.4f}")
    print(f"Test accuracy: {test_acc_rf:.4f}")
    
    # Choose best model
    if test_acc_lr > test_acc_rf:
        best_model = lr_pipeline
        best_name = "Logistic Regression"
        best_acc = test_acc_lr
        best_pred = test_pred_lr
    else:
        best_model = rf_pipeline
        best_name = "Random Forest"
        best_acc = test_acc_rf
        best_pred = test_pred_rf
    
    print(f"\n=== Best Model: {best_name} ===")
    print(f"Test Accuracy: {best_acc:.4f}")
    
    # Detailed evaluation
    print("\n=== Detailed Classification Report ===")
    print(classification_report(y_test, best_pred))
    
    # Per-class accuracy
    print("\n=== Per-Class Performance ===")
    for class_name in sorted(df['target_label'].unique()):
        class_mask = y_test == class_name
        if class_mask.sum() > 0:
            class_acc = accuracy_score(y_test[class_mask], best_pred[class_mask])
            class_count = class_mask.sum()
            print(f"{class_name}: {class_acc:.4f} accuracy ({class_count} samples)")
    
    # Complexity-based evaluation
    print("\n=== Complexity-Based Performance ===")
    
    # Get complexity labels for test set
    test_indices = X_test.index
    complexity_test = df.loc[test_indices, 'complexity']
    
    for complexity in sorted(df['complexity'].unique()):
        complexity_mask = complexity_test == complexity
        if complexity_mask.sum() > 0:
            complexity_acc = accuracy_score(y_test[complexity_mask], best_pred[complexity_mask])
            complexity_count = complexity_mask.sum()
            print(f"{complexity}: {complexity_acc:.4f} accuracy ({complexity_count} samples)")
    
    # Save the best model
    model_filename = f'log_classifier_20k_{best_name.lower().replace(" ", "_")}.joblib'
    joblib.dump(best_model, model_filename)
    print(f"\nModel saved as: {model_filename}")
    
    # Create data splits for future use
    test_df = pd.DataFrame({
        'log_message': X_test,
        'target_label': y_test,
        'complexity': complexity_test,
        'predicted': best_pred
    })
    test_df.to_csv('test_results_20k.csv', index=False)
    
    print("\n=== Benefits of Using 20K Dataset ===")
    print("‚úÖ 4,000 samples per class (vs 39-929 in 2.3K dataset)")
    print("‚úÖ Perfect class balance")
    print("‚úÖ More robust model training")
    print("‚úÖ Better generalization")
    print("‚úÖ Higher confidence predictions")
    print(f"‚úÖ Achieved {best_acc:.1%} accuracy")
    
    return best_model, best_name, best_acc

def compare_with_2k_model():
    """
    Quick comparison with 2K model performance
    """
    print("\n=== Comparison with 2.3K Model ===")
    
    # Load 2.3K dataset and train a quick model
    df_small = pd.read_csv('logs_chunk1_improved.csv')
    
    X_small = df_small['log_message']
    y_small = df_small['target_label']
    
    # Simple train/test split for comparison
    X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(
        X_small, y_small, test_size=0.2, random_state=42, stratify=y_small
    )
    
    # Train simple model
    small_pipeline = Pipeline([
        ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2))),
        ('lr', LogisticRegression(max_iter=1000, random_state=42))
    ])
    
    small_pipeline.fit(X_train_small, y_train_small)
    small_pred = small_pipeline.predict(X_test_small)
    small_acc = accuracy_score(y_test_small, small_pred)
    
    print(f"2.3K dataset model accuracy: {small_acc:.4f}")
    print(f"20K dataset model accuracy: {best_acc:.4f}")
    print(f"Improvement: {(best_acc - small_acc)*100:.1f} percentage points")
    
    if best_acc > small_acc:
        print("üéØ WINNER: 20K dataset provides better performance!")
    else:
        print("ü§î Interesting: Smaller dataset performed better (might need more tuning)")

def demonstrate_20k_model():
    """
    Demonstrate the 20K trained model
    """
    print("\n=== Model Demonstration ===")
    
    # Load the best model
    model_files = [f for f in ['log_classifier_20k_logistic_regression.joblib', 
                              'log_classifier_20k_random_forest.joblib'] 
                  if pd.io.common.file_exists(f)]
    
    if not model_files:
        print("No trained model found. Train the model first.")
        return
    
    model = joblib.load(model_files[0])
    
    # Test with various complexity examples
    test_examples = [
        # Regex complexity
        ("User login failed for admin", "Expected: user_action"),
        ("Database connection timeout", "Expected: workflow_error"),
        ("Cache cleared successfully", "Expected: system_notification"),
        
        # BERT complexity  
        ("Payment processing encountered an authentication failure", "Expected: workflow_error"),
        ("Two-factor authentication has been successfully configured", "Expected: user_action"),
        ("Memory allocation exceeded predefined threshold limits", "Expected: system_notification"),
        
        # LLM complexity
        ("The legacy authentication mechanism will be deprecated in favor of OAuth2", "Expected: deprecation_warning"),
        ("An unexpected anomaly was detected in the transaction processing pipeline", "Expected: unclassified"),
        ("User provisioning workflow completed with enhanced security protocols", "Expected: user_action")
    ]
    
    print("Sample Predictions:")
    print("-" * 80)
    
    correct_predictions = 0
    for message, expected in test_examples:
        prediction = model.predict([message])[0]
        probabilities = model.predict_proba([message])[0]
        confidence = max(probabilities)
        
        is_correct = expected.split(": ")[1] == prediction
        if is_correct:
            correct_predictions += 1
        
        status = "‚úÖ" if is_correct else "‚ùå"
        print(f"{status} Message: {message}")
        print(f"   Predicted: {prediction} (confidence: {confidence:.3f})")
        print(f"   {expected}")
        print("-" * 80)
    
    accuracy = correct_predictions / len(test_examples)
    print(f"\nDemo Accuracy: {accuracy:.1%} ({correct_predictions}/{len(test_examples)})")

if __name__ == "__main__":
    best_model, best_name, best_acc = train_20k_model()
    compare_with_2k_model()
    demonstrate_20k_model()